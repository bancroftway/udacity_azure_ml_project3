RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced',
                                        criterion='gini', max_depth=None,
                                        max_features='log2',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_impurity_split=None,
                                        min_samples_leaf=0.01,
                                        min_samples_split=0.01,
                                        min_weight_fraction_leaf=0.0,
                                        n_estimators=25, n_jobs=1,
                                        oob_score=True, random_state=None,
                                        verbose=0, warm_start=False))],
---------------

early_termination_policy = BanditPolicy(evaluation_interval=2,slack_factor=0.2)

#TODO: Create the different params that you will be using during training
param_sampling = RandomParameterSampling({'C': uniform(0.01, 0.1, 1, 10, 100),
                                        'max_iter' : choice(50,75,100,125,150,175,200),
                                        'solver' : choice('liblinear','sag','lbfgs', 'saga')})

if "training" not in os.listdir():
    os.mkdir("./training")
shutil.copy('./train.py','./training')

sklearn_env = Environment.from_conda_specification(name="sklearn-env",file_path="./conda_dependencies.yaml")
#TODO: Create your estimator and hyperdrive config
estimator = ScriptRunConfig(source_directory='./training', compute_target = compute_target,script= 'train.py',environment= sklearn_env)

hyperdrive_run_config = HyperDriveConfig(run_config=estimator, hyperparameter_sampling=param_sampling,policy=early_termination_policy,max_total_runs=50,
                                    max_duration_minutes=30,
                                    primary_metric_name='Accuracy',
                                    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE)


['--C', '10', '--max_iter', '200', '--solver', 'liblinear']



service=Model.deploy(workspace=ws,
                    name="arvc-hyper-best-model-svc5",
                    models=[model],
                    inference_config=inference_config,
                    deployment_config=deployment_config)
service.wait_for_deployment(show_output=True)



scoring_uri = service.scoring_uri

print(f'\nservice state: {service.state}\n')
print(f'scoring URI: \n{service.scoring_uri}\n')
print(f'swagger URI: \n{service.swagger_uri}\n')

print(service.scoring_uri)
print(service.swagger_uri)




modelrequest = test[:2].values.tolist()
modelrequest = [modelrequest[0][:-1],modelrequest[1][:-1]]
modelrequest



import requests
import json

data = json.dumps({"data":modelrequest})
headers = {'Content-Type':'application/json'}

response = requests.post(scoring_uri,data,headers=headers)
print(response.text)




print('Status code:',response.status_code)

print(service.get_logs())
